{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook includes my implementation of the **softmax** function, which is used to map a real-valued vector to probabilities that sum to 1. I'll also leverage the **log-sum-exp trick** to handle overflow problems and achieve numerical stability.\n",
    "\n",
    "**Softmax**\n",
    "\n",
    "The softmax function can be defined as:\n",
    "\n",
    "$$\n",
    "S(x_i, T) = p_i = \\frac{e^{x_i / T}}{\\sum_j^N{e^{x_j / T}}}\n",
    "$$\n",
    "$$\n",
    "x, p \\in \\mathcal{R}^N\n",
    "$$\n",
    "\n",
    "where the vector of probabilities $p$ has the property\n",
    "\n",
    "$$ \n",
    "\\sum{p} = 1\n",
    "$$\n",
    "\n",
    "This is useful, for example, in multiclass logistic regression where we want our model to output a probability distribution over $N$ classes.\n",
    "\n",
    "**Log-Sum-Exp Trick**\n",
    "\n",
    "One problem with the above formula for softmax is the possibility of overflow and underflow when executing with limited numerical precision. For example, if $x = [1000, -3000, -5000]$, then we will simultaneously experience overflow *and* underflow when computing $e^{x_i}$.\n",
    "\n",
    "We can correct for this by exploiting the property of logarithms:\n",
    "\n",
    "$$\n",
    "\\log{\\sum{e^{x_i}}} = \\log{\\sum{e^{x_i} \\cdot e^{m} \\cdot e^{-m}}} = m + \\log{\\sum{e^{x_i - m}}}\n",
    "$$\n",
    "\n",
    "Because we now have a new arbitrary term m, we can prevent overflow by setting m to be the max of x. As a result, we will never have exp(some value > 0), although we may still have underflow and get one term $\\approx 0$. However, the sum will still be sensible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regular implementation - not numerically stable\n",
    "def softmax(x: torch.Tensor, temperature: float = 1) -> torch.Tensor:\n",
    "    \"\"\"Non-numerically stable implementation of the softmax function\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): input vector. Expected to be 1-D\n",
    "        temperature (float): determines distribution of probability according to high/low logits. For high\n",
    "            temperature, probability is spread more evenly, while for lower temperatures probability is assigned\n",
    "            unevenly to higher logit values\n",
    "\n",
    "    Raises:\n",
    "        ValueError: if x has greater than 1 dimension\n",
    "        ValueError: if the temperature is less than or equal to 0\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: result of softmax operation. Sum of returned vector should be 1\n",
    "    \"\"\"\n",
    "    if x.ndim > 1:\n",
    "        raise ValueError(f'x is expected to be a 1D vector, instead got shape {x.shape}')\n",
    "\n",
    "    if temperature <= 0:\n",
    "        raise ValueError(f'temperature must be in the range (0, inf), instead got {temperature}')\n",
    "\n",
    "    numerator = torch.exp(x / temperature)\n",
    "\n",
    "    denominator = numerator.sum()\n",
    "\n",
    "    probabilities = numerator / denominator\n",
    "\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lse(x: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.log(torch.sum(torch.exp(x)))\n",
    "\n",
    "def softmax_stable(x: torch.Tensor, temperature: float = 1) -> torch.Tensor:\n",
    "    \"\"\"Numerically stable implementation of the softmax function using the log-sum-exp trick\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): input vector. Expected to be 1-D\n",
    "        temperature (float): determines distribution of probability according to high/low logits. For high\n",
    "            temperature, probability is spread more evenly, while for lower temperatures probability is assigned\n",
    "            unevenly to higher logit values\n",
    "\n",
    "    Raises:\n",
    "        ValueError: if x has greater than 1 dimension\n",
    "        ValueError: if the temperature is less than or equal to 0\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: result of softmax operation. Sum of returned vector should be 1\n",
    "    \"\"\"\n",
    "    if x.ndim > 1:\n",
    "        raise ValueError(f'x is expected to be a 1D vector, instead got shape {x.shape}')\n",
    "\n",
    "    if temperature <= 0:\n",
    "        raise ValueError(f'temperature must be in the range (0, inf), instead got {temperature}')\n",
    "\n",
    "    x_temp_adjusted = x / temperature\n",
    "\n",
    "    x_adjusted = x_temp_adjusted - x_temp_adjusted.max()\n",
    "\n",
    "    lse_result = lse(x_adjusted)\n",
    "\n",
    "    probabilities = torch.exp(x_adjusted - lse_result)\n",
    "\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some inputs to test for both implementations\n",
    "x_test_cases = torch.FloatTensor([[0, 0, 0],\n",
    "                [1, 2, 3],\n",
    "                [-1000000, 1, 2],\n",
    "                [1000000, 1, 2],\n",
    "                [50000, 50000, 50000],\n",
    "                [-50000, -50000, -50000]])\n",
    "\n",
    "temperature_test_cases = [1, .5, .1, .000001, 10, 10_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over all combinations of test cases for both implementations and save the results\n",
    "softmax_results_regular = []\n",
    "\n",
    "softmax_results_stable = []\n",
    "\n",
    "for x in x_test_cases:\n",
    "    for temperature in temperature_test_cases:\n",
    "        softmax_results_regular.append(softmax(x, temperature))\n",
    "        softmax_results_stable.append(softmax_stable(x, temperature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent of softmax results summing to 1 for different implementations:       \n",
      "\t-regular: 50.0 \n",
      "\t-stable (log-sum-exp): 100.0\n"
     ]
    }
   ],
   "source": [
    "# test the quality of outputs - do they sum to 1?\n",
    "softmax_regular_sum_to_one_results = []\n",
    "softmax_stable_sum_to_one_results = []\n",
    "\n",
    "for result_regular, result_stable in zip(softmax_results_regular, softmax_results_stable):\n",
    "    # absolute tolerance for checking whether the sum of the probs is 1\n",
    "    atol = .00001\n",
    "    softmax_regular_sum_to_one_results.append(torch.isclose(result_regular.sum(), torch.tensor(1.0), atol=atol))\n",
    "    softmax_stable_sum_to_one_results.append(torch.isclose(result_stable.sum(), torch.tensor(1.0), atol=atol))\n",
    "\n",
    "softmax_regular_sum_to_one_results = torch.tensor(softmax_regular_sum_to_one_results)\n",
    "softmax_stable_sum_to_one_results = torch.tensor(softmax_stable_sum_to_one_results)\n",
    "\n",
    "p_correct_regular = softmax_regular_sum_to_one_results.float().mean().item() * 100\n",
    "p_correct_stable = softmax_stable_sum_to_one_results.float().mean().item() * 100\n",
    "\n",
    "print(f'percent of softmax results summing to 1 for different implementations: \\\n",
    "      \\n\\t-regular: {p_correct_regular} \\n\\t-stable (log-sum-exp): {p_correct_stable}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
